{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['files', 'find', 'my', 'files']\n",
    "\n",
    "n_words = len(set(words))\n",
    "words_keys = {}\n",
    "for i, w in enumerate(words):\n",
    "    if w not in words_keys:\n",
    "        words_keys[w] = i\n",
    "\n",
    "output = torch.empty((n_words,0), dtype=torch.float32)\n",
    "\n",
    "for word in words:\n",
    "    one_hot = torch.zeros(n_words)\n",
    "    one_hot[words_keys[word]] = 1\n",
    "    output = torch.column_stack((output, one_hot))\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0,1,1,2]).float()\n",
    "b = torch.tensor([1,0,1,2]).float()\n",
    "\n",
    "print(a @ b)\n",
    "\n",
    "a = torch.tensor([0,1,0,0]).float()\n",
    "b = torch.tensor([0,0,0,1]).float()\n",
    "\n",
    "print(a @ b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([ 0, 0, 1, 0], dtype=torch.float32)\n",
    "b = torch.tensor([.2,.7,.8,.1]).t()\n",
    "\n",
    "print(a @ b)\n",
    "\n",
    "a = torch.tensor(([ 1, 0, 0, 0],\n",
    "                  [ 0, 0, 1, 0]) , dtype=torch.float32)\n",
    "b = torch.tensor([.2,.7,.8,.1]).t()\n",
    "\n",
    "print(a @ b)\n",
    "\n",
    "\n",
    "a = torch.tensor([ 0, 0, 1, 0] , dtype=torch.float32)\n",
    "b = torch.tensor(([.2,.7,.8,.1],\n",
    "                 [.9, 0,.3,.4])).t()\n",
    "print(a @ b)\n",
    "\n",
    "a = torch.tensor(([ 1, 0, 0, 0],\n",
    "                  [ 0, 0, 0, 1],\n",
    "                  [ 0, 0, 1, 0]) , dtype=torch.float32)\n",
    "b = torch.tensor(([.2,.7,.8,.1],\n",
    "                  [.9, 0,.3,.4])).t()\n",
    "print(a @ b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First order sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import pairwise, product\n",
    "            # Phrase                         Probability\n",
    "commands = [(\"show me my directories please\", .2),\n",
    "            (\"show me my files please\",       .3),\n",
    "            (\"show me my photos please\",      .5)]\n",
    "\n",
    "vocabulary = set([j for i in commands for j in i[0].split()])\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "\n",
    "voc_keys = {w:i for i,w in enumerate(sorted(vocabulary))}\n",
    "print(voc_keys)\n",
    "\n",
    "matrix = torch.zeros(len(vocabulary), len(vocabulary))\n",
    "\n",
    "\n",
    "for phrase, probability in commands:\n",
    "    for word, next_word in pairwise(phrase.split()):\n",
    "    #    print(word,next_word, probability, voc_keys[word], voc_keys[next_word])\n",
    "       matrix[(voc_keys[word], voc_keys[next_word])] += probability\n",
    "\n",
    "for i,l in enumerate(matrix):\n",
    "    if ((a:=sum(l)) != 0):\n",
    "        matrix[i]/=a \n",
    "\n",
    "\n",
    "print(\" \"*11,end=\" \")\n",
    "for word in sorted(vocabulary):\n",
    "    print(f\"{word[:6]:6}\",end=\" \")\n",
    "print()\n",
    "for word1 in sorted(vocabulary):\n",
    "    print(f\"{word1:11}\",end=\" \")\n",
    "    for word2 in sorted(vocabulary):\n",
    "        print(f\"{matrix[voc_keys[word1], voc_keys[word2]].item():<6.2f}\",end=\" \")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import pairwise, combinations\n",
    "            # Phrase                         Probability\n",
    "commands = [(\"check whether the battery ran down please\", 0.4),\n",
    "            (\"check whether the program ran please\",      0.6)]\n",
    "\n",
    "vocabulary = {j for i in commands for j in i[0].split()}\n",
    "# print(\"Vocabulary:\", vocabulary)\n",
    "\n",
    "voc_keys = {w:i for i,w in enumerate(vocabulary)}\n",
    "# print(voc_keys)\n",
    "\n",
    "words_combinations = {c: frozenset(c) for c in sorted(combinations(vocabulary,2))}\n",
    "\n",
    "matrix = torch.zeros(len(words_combinations), len(vocabulary))\n",
    "\n",
    "comb_keys = {w:i for i,w in enumerate(words_combinations.values())}\n",
    "\n",
    "\n",
    "for phrase, probability in commands:\n",
    "    for *comb, next_word in zip(*[phrase.split()[i:] for i in range(3)]):\n",
    "       matrix[(comb_keys[frozenset(comb)], voc_keys[next_word])] += probability\n",
    "\n",
    "for i,l in enumerate(matrix):\n",
    "    if ((a:=sum(l)) != 0):\n",
    "        matrix[i]/=a \n",
    "\n",
    "print(\" \"*15,end=\" \")\n",
    "for word in sorted(vocabulary):\n",
    "    print(f\"{word[:6]:6}\",end=\" \")\n",
    "print()\n",
    "\n",
    "# sorted_comb = {tuple(fz): fz for fz in words_combinations}\n",
    "for comb, fz in words_combinations.items():\n",
    "    print(f\"{comb[0]+' '+comb[1]:15}\",end=\" \")\n",
    "    for word in sorted(vocabulary):\n",
    "        print(f\"{matrix[comb_keys[fz], voc_keys[word]].item():<6.2f}\",end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import combinations_with_replacement\n",
    "\n",
    "            # Phrase                                                          Probability\n",
    "commands = [(\"check the program log and find out whether it ran please\",      0.5),\n",
    "            (\"check the battery log and find out whether it ran down please\", 0.5)]\n",
    "vocabulary = {j for i in commands for j in i[0].split()} \n",
    "voc_keys = {w:i for i,w in enumerate(vocabulary)}\n",
    "\n",
    "ran_combinations = {(j, 'ran') for i in commands for j in i[0].split()[:i[0].split().index('ran')+1]}\n",
    "comb_keys = {w:i for i,w in enumerate(ran_combinations)}\n",
    "\n",
    "matrix = torch.zeros(len(ran_combinations), len(vocabulary))\n",
    "\n",
    "for phrase, probability in commands:\n",
    "    phrase = phrase.split();\n",
    "    ran_index = phrase.index('ran')+1\n",
    "    for word in phrase[:ran_index]:\n",
    "        matrix[(comb_keys[(word, 'ran')], voc_keys[phrase[ran_index]])] += probability\n",
    "\n",
    "for i,l in enumerate(matrix):\n",
    "    if ((a:=sum(l)) != 0):\n",
    "        matrix[i]/=a \n",
    "\n",
    "# Priting\n",
    "def print_matrix():\n",
    "    margin = max(len(comb[0]+' '+comb[1]) for comb in ran_combinations)\n",
    "    print(\" \"*margin,end=\" \")\n",
    "    for word in sorted(vocabulary):\n",
    "        print(f\"{word[:6]:6}\",end=\" \")\n",
    "    print()\n",
    "    for comb in sorted(ran_combinations):\n",
    "        print(f\"{comb[0]+' '+comb[1]:{margin}}\",end=\" \")\n",
    "        for word in sorted(vocabulary):\n",
    "            print(f\"{matrix[comb_keys[comb], voc_keys[word]].item():<6.2f}\",end=\" \")\n",
    "        print()\n",
    "\n",
    "print_matrix()\n",
    "print()\n",
    "\n",
    "mask = torch.zeros(len(ran_combinations))\n",
    "mask[comb_keys[('battery', 'ran')]] = 1\n",
    "mask[comb_keys[('program', 'ran')]] = 1\n",
    "matrix = (matrix.T * mask).T\n",
    "print_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p xmlns:cc=\"http://creativecommons.org/ns#\" >This work is licensed under <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1\" target=\"_blank\" rel=\"license noopener noreferrer\" style=\"display:inline-block;\">CC BY-NC-SA 4.0<img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1\"></a></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "from math import ceil\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(functional.relu(self.w_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.Wv = nn.Linear(d_model, d_v)  \n",
    "        self.Wk = nn.Linear(d_model, d_k)  \n",
    "        self.Wq = nn.Linear(d_model, d_k)\n",
    "\n",
    "    def forward(self, v, k, q, mask):\n",
    "        v = self.Wv(v)\n",
    "        k = self.Wv(k)\n",
    "        q = self.Wv(q)\n",
    "\n",
    "        qk = q@k.transpose(1, 2)\n",
    "        qk = qk/self.d_k\n",
    "        if mask is not None:\n",
    "            mask = mask.reshape(qk.shape[0], 1, qk.shape[2])\n",
    "            qk = qk.masked_fill(mask == 0, -1e9)\n",
    "        qk = torch.softmax(qk, dim=-1)\n",
    "\n",
    "        qkv = qk @ v\n",
    "        return qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h, mask: bool = False, n:int = None):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.Wv = torch.nn.ModuleList([nn.Linear(d_model, d_v)]*h)  # value matrices\n",
    "        self.Wk = torch.nn.ModuleList([nn.Linear(d_model, d_k)]*h)  # key matrices\n",
    "        self.Wq = torch.nn.ModuleList([nn.Linear(d_model, d_k)]*h)  # query matrices\n",
    "        self.lineal = nn.Linear(h*d_v, d_model)\n",
    "        if mask:\n",
    "            self.mask = torch.ones((n,n))\n",
    "            self.mask = torch.tril(self.mask)\n",
    "    def forward(self, v, k ,q, mask = None):\n",
    "        nv = [w(v) for w in self.Wv]\n",
    "        nk = [w(k) for w in self.Wk]\n",
    "        nq = [w(q) for w in self.Wq]\n",
    "\n",
    "        att = [self.atention(*i, mask) for i in zip(nv,nq, nk)]\n",
    "        att =torch.concat(att, dim=-1)\n",
    "        return self.lineal(att)\n",
    "\n",
    "    def atention(self, v, k, q, mask):\n",
    "        qk = q@k.transpose(1, 2)\n",
    "        qk = qk/self.d_k\n",
    "        if mask is not None:\n",
    "            mask = mask.reshape(qk.shape[0], 1, qk.shape[2])\n",
    "            qk = qk.masked_fill(mask == 0, -1e9)\n",
    "        qk = torch.softmax(qk, dim=-1)\n",
    "\n",
    "        qkv = qk @ v\n",
    "        return qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNormalize(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(AddNormalize, self).__init__()\n",
    "      \n",
    "  def forward(self, x, y):\n",
    "    x = x + y\n",
    "    mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "    std = torch.std(x, dim=-1, keepdim=True)\n",
    "    return (x-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h, d_ff):\n",
    "      '''\n",
    "      N: Número de palabras del vocabulario\n",
    "      d_model: Número de dimensiones de embdedding de vocabulario\n",
    "      d_k: Número de dimensiones del embdedding de claves/consultas\n",
    "      d_v: Número de dimensiones de embdedding de valores\n",
    "      d_v: Número de dimensiones de embdedding de valores\n",
    "      h: Número de cabezas de atención\n",
    "      '''\n",
    "      super(Encoder, self).__init__()\n",
    "      self.encoder_mh = MultiheadAttention(d_model, d_k, d_v, h)\n",
    "      self.norm       = AddNormalize()\n",
    "      self.encoder_ff = FeedForward(d_model, d_ff)\n",
    "    # x represents our data\n",
    "    def forward(self, inputs):\n",
    "      enc_mha   = self.encoder_mh(inputs, inputs, inputs)\n",
    "      enc_mha   = self.norm(inputs, enc_mha)\n",
    "      enc_ff    = self.encoder_ff(enc_mha)\n",
    "      enc_ff    = self.norm(enc_ff, enc_mha)\n",
    "      return enc_ff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h, d_ff, n, features=True):\n",
    "      '''\n",
    "      N: Número de palabras del vocabulario\n",
    "      d_model: Número de dimensiones de embdedding de vocabulario\n",
    "      d_k: Número de dimensiones del embdedding de claves/consultas\n",
    "      d_v: Número de dimensiones de embdedding de valores\n",
    "      d_v: Número de dimensiones de embdedding de valores\n",
    "      h: Número de cabezas de atención\n",
    "      '''\n",
    "      super(Decoder, self).__init__()\n",
    "      self.masked_mh  = MultiheadAttention(d_model, d_k, d_v, h)\n",
    "      if features:\n",
    "        self.decoder_mh = MultiheadAttention(d_model, d_k, d_v, h)\n",
    "      self.decoder_ff = FeedForward(d_model, d_ff)  \n",
    "      self.norm       = AddNormalize()\n",
    "      \n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "    # x represents our data\n",
    "    def forward(self, outputs, mask, features=None):\n",
    "      dec_mmha   = self.masked_mh(outputs, outputs, outputs, mask)\n",
    "      dec_mmha   = self.norm(outputs, dec_mmha)\n",
    "      if features:\n",
    "        dec_mha    = self.decoder_mh(features, features, dec_mmha)\n",
    "        dec_mha    = self.norm(dec_mmha, dec_mha)\n",
    "      else:\n",
    "        dec_mha = dec_mmha\n",
    "      dec_ff     = self.decoder_ff(dec_mha)\n",
    "\n",
    "      if self.training:\n",
    "        dec_ff = self.dropout(dec_ff)\n",
    "      dec_ff     = self.norm(dec_ff, dec_mha)\n",
    "      return dec_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, n):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        # self.pe = torch.zeros((n, d_model))\n",
    "        # position = torch.arange(0, n)\n",
    "        # i = torch.torch.arange(0, d_model, 2)\n",
    "        # div_term = 10_000 ** (2*i/d_model)\n",
    "        # self.pe[:, 0::2] = torch.sin(position / div_term)\n",
    "        # self.pe[:, 1::2] = torch.cos(position / div_term)[:,:-1 if d_model%2 else None]\n",
    "        # self.pe = self.pe.unsqueeze(0)\n",
    "\n",
    "        positional_encoding = np.zeros((n, d_model))\n",
    "\n",
    "        # Calculate positional encoding for each position and each dimension\n",
    "        for pos in range(n):\n",
    "            for i in range(0, d_model, 2):\n",
    "                # Apply sin to even indices in the array; indices in Python start at 0 so i is even.\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10_000 ** ((2 * i) / d_model)))\n",
    "\n",
    "                if i + 1 < d_model:\n",
    "                    # Apply cos to odd indices in the array; we add 1 to i because indices in Python start at 0.\n",
    "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / d_model)))\n",
    "\n",
    "        # Convert numpy array to PyTorch tensor and return it\n",
    "        self.pe = torch.from_numpy(positional_encoding).float().cuda()\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        return x + self.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N, d_model, d_k, d_v, h, d_ff, n):\n",
    "      '''\n",
    "      N: Número de palabras del vocabulario\n",
    "      d_model: Número de dimensiones de embdedding de vocabulario\n",
    "      d_k: Número de dimensiones del embdedding de claves/consultas\n",
    "      d_v: Número de dimensiones de embdedding de valores\n",
    "      d_v: Número de dimensiones de embdedding de valores\n",
    "      h: Número de cabezas de atención\n",
    "      '''\n",
    "      super(Transformer, self).__init__()\n",
    "\n",
    "      self.encoder_em = nn.Embedding(N, d_model)\n",
    "      self.posencoding = PositionalEncoding(d_model, n)\n",
    "      self.decoder_em = nn.Embedding(N, d_model)\n",
    "      self.encoder    = Encoder(d_model, d_k, d_v, h, d_ff) \n",
    "      self.decoder    = Decoder(d_model, d_k, d_v, h, d_ff, n) \n",
    "      self.output     = nn.Linear(d_model, N)\n",
    "    # x represents our data\n",
    "    def forward(self, inputs, outputs):\n",
    "      emb_input = self.encoder_em(inputs)\n",
    "      emb_input = self.posencoding(emb_input)\n",
    "      features  = self.encoder(emb_input)\n",
    "\n",
    "      emb_output = self.encoder_em(outputs)\n",
    "      emb_output = self.posencoding(emb_output)\n",
    "      dec_out    = self.decoder(emb_output, features)\n",
    "\n",
    "      output     = self.output(dec_out)\n",
    "      return torch.softmax(output, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, N, d_model, d_k, d_v, h, d_ff, n, n_dec_bloks=1):\n",
    "      '''\n",
    "      N: Número de palabras del vocabulario\n",
    "      d_model: Número de dimensiones de embdedding de vocabulario\n",
    "      d_k: Número de dimensiones del embdedding de claves/consultas\n",
    "      d_v: Número de dimensiones de embdedding de valores\n",
    "      d_v: Número de dimensiones de embdedding de valores\n",
    "      h: Número de cabezas de atención\n",
    "      '''\n",
    "      super(TextGenerator, self).__init__()\n",
    "\n",
    "      self.decoder_em = nn.Embedding(N, d_model)\n",
    "      self.posencoding = PositionalEncoding(d_model, n)\n",
    "      self.decoder    = torch.nn.ModuleList([Decoder(d_model, d_k, d_v, h, d_ff, n, features=False)]*n_dec_bloks)\n",
    "      self.output     = nn.Linear(d_model, N)\n",
    "    # x represents our data\n",
    "    def forward(self, inputs, mask):\n",
    "      emb_output = self.decoder_em(inputs)\n",
    "      output = self.posencoding(emb_output)\n",
    "      for dec in self.decoder:\n",
    "        output = dec(output, mask)\n",
    "\n",
    "      output = self.output(output)\n",
    "      # return torch.softmax(output, dim=-1)\n",
    "      return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveWrapper(torch.nn.Module):\n",
    "    def __init__(self, gpt_model , max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.model = gpt_model\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        inp, target = x[:, :-1], x[:, 1:]\n",
    "        mask = mask[:, :-1]\n",
    "\n",
    "        output = self.model(inp, mask)\n",
    "        return output, target\n",
    "\n",
    "    def next_token_probabilities(self, x, mask, temperature=1.0):\n",
    "        logits = self.model(x, mask)[:, -1]\n",
    "\n",
    "        # Apply the temperature\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "        # Apply the softmax\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dictionary = {}\n",
    "        self.reverse_dictionary = []\n",
    "        # Add the padding token\n",
    "        self._add_to_dict('<PAD>')\n",
    "\n",
    "        # Add characters and numbers to the dictionary\n",
    "        for i in range(10):\n",
    "            self._add_to_dict(str(i))\n",
    "        for i in range(26):\n",
    "            self._add_to_dict(chr(ord('a') + i))\n",
    "\n",
    "        # Add space and punctuation to the dictionary\n",
    "        self._add_to_dict('.')\n",
    "        self._add_to_dict(' ')\n",
    "\n",
    "    def _add_to_dict(self, character):\n",
    "        if character not in self.dictionary:\n",
    "            index = len(self.dictionary)\n",
    "            self.dictionary[character] = index\n",
    "            self.reverse_dictionary.append(character)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [self.dictionary[c] for c in text]\n",
    "\n",
    "    def character_to_token(self, character):\n",
    "        return self.dictionary[character]\n",
    "\n",
    "    def token_to_character(self, token):\n",
    "        return self.reverse_dictionary[token]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, tokenizer: Tokenizer, optimizer=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        if optimizer is None:\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "    def __gen_batch(self, data, batch_size):\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            sequence_tensor = torch.tensor(data[i: i + batch_size], dtype=torch.long)\n",
    "\n",
    "            # Create the mask tensor for the batch, where 1 means the token is not a padding token\n",
    "            mask_tensor = torch.ones_like(sequence_tensor)\n",
    "            mask_tensor[sequence_tensor == self.tokenizer.character_to_token('<PAD>')] = 0\n",
    "\n",
    "            yield (sequence_tensor, mask_tensor)\n",
    "    def train(self, data: list[str], epochs, batch_size):\n",
    "        loss_per_epoch = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "\n",
    "            # Shuffle the sequences\n",
    "            random.shuffle(data)\n",
    "\n",
    "            # Create batches of sequences and their respective mask.\n",
    "\n",
    "            # Train the model on each batch\n",
    "            for batch_number, batch in enumerate(self.__gen_batch(data, batch_size)):\n",
    "                self.model.train()\n",
    "\n",
    "                # Create the input and mask tensors\n",
    "                input_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
    "                mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
    "\n",
    "                for i, input_entry in enumerate(batch[0]):\n",
    "                    input_tensor[i] = input_entry\n",
    "\n",
    "                for i, mask_entry in enumerate(batch[1]):\n",
    "                    mask_tensor[i] = mask_entry\n",
    "\n",
    "                # Compute the model output\n",
    "                model_output, target = self.model(input_tensor, mask_tensor)\n",
    "\n",
    "                # Compute the losses\n",
    "                # The loss is computed on the model output and the target\n",
    "                loss = self.loss_function(model_output.transpose(1, 2), target)\n",
    "\n",
    "                print(f\"{batch_number}/{len(data)//batch_size} Loss: {loss}\", end=\"\\r\")\n",
    "                # Backpropagate the loss.\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip the gradients. This is used to prevent exploding gradients.\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "\n",
    "                # Update the model parameters. This is done by taking a step in the direction of the gradient.\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Reset the gradients. This is done so that the gradients from the previous batch\n",
    "                # are not used in the next step.\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Append the loss to the list of losses, so that the average loss can be computed for this epoch.\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            # Print the loss\n",
    "            epoch_loss = np.average(losses)\n",
    "            loss_per_epoch.append(epoch_loss)\n",
    "            print('Epoch:', epoch, 'Loss:', epoch_loss)\n",
    "\n",
    "        return loss_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sequences(max_sequence_length, tokenized_training_data):\n",
    "    # Create sequences of length max_sequence_length + 1\n",
    "    # The last token of each sequence is the target token\n",
    "    sequences = []\n",
    "    for i in range(0, len(tokenized_training_data) - max_sequence_length - 1):\n",
    "        sequences.append(tokenized_training_data[i: i + max_sequence_length + 1])\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def tokenize_and_PAD_training_data(max_sequence_length, tokenizer, training_data):\n",
    "    # Tokenize the training data\n",
    "    tokenized_training_data = tokenizer.tokenize(training_data)\n",
    "    for _ in range(max_sequence_length):\n",
    "        # Prepend PADding tokens\n",
    "        tokenized_training_data.insert(0, tokenizer.character_to_token('<PAD>'))\n",
    "    return tokenized_training_data\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "embedding_dimension = 256\n",
    "max_sequence_length = 20\n",
    "number_of_tokens = tokenizer.size()\n",
    "\n",
    "# Create the model\n",
    "model = AutoregressiveWrapper(TextGenerator(number_of_tokens, embedding_dimension, \n",
    "            64, 64, 4, 256//4, max_sequence_length, 3), max_sequence_length)\n",
    "# Create the training data\n",
    "training_data = '. '.join([\n",
    "    'cats rule the world',\n",
    "    'dogs are the best',\n",
    "    'elephants have long trunks',\n",
    "    'monkeys like bananas',\n",
    "    'pandas eat bamboo',\n",
    "    'tigers are dangerous',\n",
    "    'zebras have stripes',\n",
    "    'lions are the kings of the savannah',\n",
    "    'giraffes have long necks',\n",
    "    'hippos are big and scary',\n",
    "    'rhinos have horns',\n",
    "    'penguins live in the arctic',\n",
    "    'polar bears are white'\n",
    "])\n",
    "\n",
    "tokenized_and_PADded_training_data = tokenize_and_PAD_training_data(max_sequence_length, tokenizer, training_data)\n",
    "sequences = create_training_sequences(max_sequence_length, tokenized_and_PADded_training_data)\n",
    "\n",
    "# Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "trainer = Trainer(model, tokenizer, optimizer)\n",
    "loss = trainer.train(sequences, epochs=120, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sequences(max_sequence_length, tokenized_training_data):\n",
    "    # Create sequences of length max_sequence_length + 1\n",
    "    # The last token of each sequence is the target token\n",
    "    sequences = []\n",
    "    for i in range(0, len(tokenized_training_data) - max_sequence_length - 1):\n",
    "        sequences.append(tokenized_training_data[i: i + max_sequence_length + 1])\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def tokenize_and_PAD_training_data(max_sequence_length, tokenizer, training_data):\n",
    "    # Tokenize the training data\n",
    "    tokenized_training_data = tokenizer.tokenize(training_data)\n",
    "    for _ in range(max_sequence_length):\n",
    "        # Prepend PADding tokens\n",
    "        tokenized_training_data.insert(0, tokenizer.character_to_token('<PAD>'))\n",
    "    return tokenized_training_data\n",
    "\n",
    "\n",
    "# tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "# Create the training data\n",
    "training_data = '. '.join([\n",
    "    'cats rule'\n",
    "])\n",
    "\n",
    "tokenized_and_PADded_training_data = tokenize_and_PAD_training_data(max_sequence_length, tokenizer, training_data)\n",
    "sequences = create_training_sequences(max_sequence_length, tokenized_and_PADded_training_data)\n",
    "\n",
    "# print(tokenized_and_PADded_training_data)\n",
    "tensor = torch.tensor([tokenized_and_PADded_training_data[9:]])\n",
    "\n",
    "mask = torch.ones_like(tensor)\n",
    "mask[tensor == 0] = 0\n",
    "\n",
    "model.eval()\n",
    "out=model.next_token_probabilities(tensor, mask)\n",
    "# token = random.choices(range(tokenizer.size()), out)[0]\n",
    "token = torch.multinomial(out, num_samples=1)\n",
    "print(tokenizer.token_to_character(token), end=\"\")\n",
    "while 1:\n",
    "    tensor = torch.cat([tensor, token], dim=1)\n",
    "    tensor = tensor[:, 1:]\n",
    "    mask = torch.ones_like(tensor)\n",
    "    mask[tensor == 0] = 0\n",
    "    out=model.next_token_probabilities(tensor, mask)\n",
    "    token = torch.multinomial(out, num_samples=1)\n",
    "    print(tokenizer.token_to_character(token), end=\"\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28menumerate\u001b[39m(i)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, j \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(i)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mmost_common\u001b[49m:\n\u001b[0;32m     28\u001b[0m         new_text[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(new_index)\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28mnext\u001b[39m(iterator)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import  Counter\n",
    "from itertools import pairwise  \n",
    "with open(\"esto va a salir mal.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read().split('------------')\n",
    "\n",
    "vocabulary = set()\n",
    "[vocabulary.update(i) for i in text]\n",
    "vocabulary = ['<PAD>', \"<EOS>\"] + list(vocabulary)\n",
    "indexes = {j:i for i,j in enumerate(vocabulary)}\n",
    "text = [[indexes[j] for j in i] for i in text]\n",
    "\n",
    "# print(text)\n",
    "\n",
    "while len(vocabulary) < 1024:\n",
    "    count = Counter()\n",
    "    [count.update(pairwise(i)) for i in text]\n",
    "    most_common, _ = max(count.items(), key= lambda x: x[1])\n",
    "    most_common_str = \"\".join(vocabulary[i] for i in most_common)\n",
    "    vocabulary.append(most_common_str)\n",
    "    new_index= len(indexes)\n",
    "    indexes[most_common_str] = new_index\n",
    "    new_text=[]\n",
    "    for i in text:\n",
    "        new_text.append([])\n",
    "        iterator = enumerate(i)\n",
    "        for idx, j in iterator:\n",
    "            if idx < len(i)-1 and (j, i[idx+1])==most_common:\n",
    "                new_text[-1].append(new_index)\n",
    "                next(iterator)\n",
    "            else:\n",
    "               new_text[-1].append(j)  \n",
    "    text = new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import pairwise\n",
    "\n",
    "class TokenizerBPE(Tokenizer):\n",
    "    def __init__(self, N):\n",
    "        super(TokenizerBPE, self).__init__()\n",
    "        self.dictionary = {}\n",
    "        self.reverse_dictionary = []\n",
    "        # Add the PADding token\n",
    "        self._add_to_dict('<PAD>')\n",
    "        self._add_to_dict('\\x00')\n",
    "        self.N = N\n",
    "\n",
    "    def add_list(self, text_list):\n",
    "        [self._add_to_dict(character) for text in text_list for character in text]\n",
    "        text_list = [self.tokenize(i) for i in text_list]\n",
    "\n",
    "        while len(self.dictionary) < self.N:\n",
    "            count = Counter()\n",
    "            [count.update(pairwise(i)) for i in text_list]\n",
    "            most_common, _ = count.most_common(1)[0]\n",
    "            most_common_str = \"\".join(self.reverse_dictionary[i] for i in most_common)\n",
    "            self.reverse_dictionary.append(most_common_str)\n",
    "            new_index= len(self.dictionary)\n",
    "            self.dictionary[most_common] = new_index\n",
    "            new_text=[]\n",
    "            for i in text_list:\n",
    "                new_text.append([])\n",
    "                iterator = enumerate(i)\n",
    "                for idx, j in iterator:\n",
    "                    if idx < len(i)-1 and (j, i[idx+1])==most_common:\n",
    "                        new_text[-1].append(new_index)\n",
    "                        next(iterator)\n",
    "                    else:\n",
    "                        new_text[-1].append(j)  \n",
    "            text_list = new_text\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = [self.dictionary[c] for c in text]\n",
    "        changes = True \n",
    "        while changes:\n",
    "            new_text = []\n",
    "            iterator = enumerate(text)\n",
    "            changes = False\n",
    "            for idx, i in iterator:\n",
    "                if idx < len(text)-1 and (i, text[idx+1]) in self.dictionary:\n",
    "                    new_text.append(self.dictionary[(i, text[idx+1])])\n",
    "                    next(iterator)\n",
    "                    changes = True\n",
    "                else:\n",
    "                    new_text.append(i)\n",
    "            text=new_text\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created tokenization\n"
     ]
    }
   ],
   "source": [
    "def create_training_sequences(max_sequence_length, tokenized_training_data):\n",
    "    # Create sequences of length max_sequence_length + 1\n",
    "    # The last token of each sequence is the target token\n",
    "    sequences = []\n",
    "    for i in range(0, len(tokenized_training_data) - max_sequence_length - 1):\n",
    "        sequences.append(tokenized_training_data[i: i + max_sequence_length + 1])\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def tokenize_and_PAD_training_data(max_sequence_length, tokenizer, training_data):\n",
    "    # Tokenize the training data\n",
    "    tokenized_training_data = tokenizer.tokenize(training_data)\n",
    "    for _ in range(max_sequence_length):\n",
    "        # Prepend PADding tokens\n",
    "        tokenized_training_data.insert(0, tokenizer.character_to_token('<PAD>'))\n",
    "    return tokenized_training_data\n",
    "\n",
    "\n",
    "max_sequence_length = 512\n",
    "number_of_tokens = 1024\n",
    "\n",
    "# Load the training data\n",
    "with open(\"esto va a salir mal.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read().split('------------')\n",
    "\n",
    "tokenizer = TokenizerBPE(number_of_tokens)\n",
    "tokenizer.add_list(text)\n",
    "\n",
    "print(\"Created tokenization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 64\n",
    "\n",
    "\n",
    "model = AutoregressiveWrapper(TextGenerator(number_of_tokens, embedding_dimension, \n",
    "            8, 8, 8, 256//4, max_sequence_length, 4), max_sequence_length)\n",
    "\n",
    "training_data = '\\x00'.join(text)\n",
    "\n",
    "tokenized_and_PADded_training_data = tokenize_and_PAD_training_data(max_sequence_length, tokenizer, training_data)\n",
    "sequences = create_training_sequences(max_sequence_length, tokenized_and_PADded_training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373/1940 Loss: 3.1226003170013428\r"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "trainer = Trainer(model, tokenizer, optimizer)\n",
    "loss = trainer.train(sequences, epochs=1200, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_statedict\")\n",
    "torch.save(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lolodia, el usuoagenlotieneconaseste, exs\n",
      "Que sororejorazónnntereoja;\n",
      "sulio.\n",
      "\n",
      "Y que en pagunonajaro y.\n",
      "Y loceo que siiena.\n",
      "meóno.\n",
      "\n",
      "Y poonirme, era al cientoqué con diosguasfioíjo.\n",
      "Vámeno.\n",
      "di\n",
      "iguotazuorcaslodo.\n",
      "\n",
      "Lerd, no? yo algeso, no\n",
      "y un,\n",
      "hacensé qué bín tuorojereso, empiezo a duea.\n",
      "tivo.\n",
      "¡que esguejasa el e caonalo nitoo\n",
      "y envezono, yo, evo sionegn veoorerounay, no,\n",
      "siego rego.\n",
      "Soy entenrisorermea,\n",
      "quej, y noy en ego eva en y, todo.\n",
      "\n",
      "Saquí.\n",
      "Destepa; en esforío.\n",
      "mi nsolal, tendo.\n",
      "Cmeterdaado\n",
      "podefriaba la encuenoríjartendónmás\n",
      "Dino, ohe luz demasinga, lasa!\n",
      "con una blno, y nas go el mueriitodo,\n",
      "Me acueemaya.\n",
      "ndo, masia, suusmorriego el preguego suextienó siencivo.\n",
      "Me mayzhe.\n",
      "yo qurisa nano, alegría a ase, no.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_prompt(max_sequence_length, tokenizer, training_data):\n",
    "    # Tokenize the training data\n",
    "    tokenized_training_data = tokenizer.tokenize(training_data)\n",
    "    for _ in range(max_sequence_length - len(tokenized_training_data)):\n",
    "        # Prepend PADding tokens\n",
    "        tokenized_training_data.insert(0, tokenizer.character_to_token('<PAD>'))\n",
    "    return tokenized_training_data\n",
    "\n",
    "\n",
    "# Create the training data\n",
    "prompt = input(\">\")\n",
    "\n",
    "tokenized_and_PADded_training_data = tokenize_prompt(max_sequence_length, tokenizer, prompt)\n",
    "\n",
    "# print(tokenized_and_PADded_training_data)\n",
    "tensor = torch.tensor([tokenized_and_PADded_training_data])\n",
    "\n",
    "model.eval()\n",
    "while 1:\n",
    "    mask = torch.ones_like(tensor)\n",
    "    mask[tensor == 0] = 0\n",
    "\n",
    "    out=model.next_token_probabilities(tensor, mask)\n",
    "    token = torch.multinomial(out, num_samples=1)\n",
    "    if token == tokenizer.character_to_token('\\x00'):\n",
    "        break\n",
    "    print(tokenizer.token_to_character(token), end=\"\")\n",
    "    tensor = torch.cat([tensor, token], dim=1)\n",
    "    tensor = tensor[:, 1:]\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
